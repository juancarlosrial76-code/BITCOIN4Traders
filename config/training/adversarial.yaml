
# Adversarial Training Configuration
# ==================================
# Defines hyperparameters for:
# - Training loop
# - Trader agent (PPO)
# - Adversary agent (PPO)

# Main Training Loop
# Mathematically sound: 500 Iters = 53x data passes (19260 samples)
# Adversary starts at 100 → Trader has enough warm-up
training:
  n_iterations: 500
  steps_per_iteration: 2048
  save_frequency: 50
  log_frequency: 10
  checkpoint_dir: "data/models/adversarial"
  adversary_start_iteration: 100
  adversary_strength: 0.1

# Trader Agent Configuration
# FIXES:
#   - entropy_coef: 0.01 → 0.03  (prevents policy collapse after Iter ~11)
#   - actor_lr: 3e-4 → 1e-4      (slower learning = more stable policy)
#   - target_kl: 0.01 → 0.015    (less aggressive early stopping)
#   - lr_decay_gamma: 0.99 → 0.995 (LR decays more slowly)
trader:
  # Network Architecture
  hidden_dim: 128
  use_recurrent: true
  rnn_type: "GRU"
  rnn_layers: 1
  dropout: 0.1
  use_layer_norm: true
  
  # Learning
  actor_lr: 1.0e-4      # was 3e-4 → too high → fast entropy collapse
  critic_lr: 5.0e-4     # was 1e-3 → too high for Critic stability
  use_lr_decay: true
  lr_decay_gamma: 0.995  # was 0.99 → LR drops to 8% of start after 500 Iters
  
  # PPO
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  n_epochs: 10
  batch_size: 64
  entropy_coef: 0.08    # increased: prevents collapse onto a single dominant action
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.015      # was 0.01 → slightly looser for early training

# Adversary Agent Configuration
adversary:
  # Network Architecture
  hidden_dim: 128
  use_recurrent: true
  rnn_type: "GRU"
  
  # Learning (slower than Trader for stability)
  actor_lr: 5.0e-5      # was 1e-4
  critic_lr: 2.0e-4     # was 5e-4
  use_lr_decay: true
  
  # PPO
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  n_epochs: 10
  batch_size: 64
  entropy_coef: 0.05    # was 0.02 → Adversary needs more exploration
