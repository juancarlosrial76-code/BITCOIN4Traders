
# Adversarial Training Configuration
# ==================================
# Defines hyperparameters for:
# - Training loop
# - Trader agent (PPO)
# - Adversary agent (PPO)

# Main Training Loop
training:
  n_iterations: 500
  steps_per_iteration: 2048
  save_frequency: 50
  log_frequency: 10
  checkpoint_dir: "data/models/adversarial"
  adversary_start_iteration: 100
  adversary_strength: 0.1

# Trader Agent Configuration (optimized for profit)
trader:
  # Network Architecture
  hidden_dim: 128
  use_recurrent: true
  rnn_type: "GRU"
  rnn_layers: 1
  dropout: 0.1
  use_layer_norm: true
  
  # Learning
  actor_lr: 3.0e-4
  critic_lr: 1.0e-3
  use_lr_decay: true
  lr_decay_gamma: 0.99
  
  # PPO
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  n_epochs: 10
  batch_size: 64
  entropy_coef: 0.01
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.01

# Adversary Agent Configuration (optimized for difficulty)
adversary:
  # Network Architecture
  hidden_dim: 128
  use_recurrent: true  # Adversary also benefits from memory
  rnn_type: "GRU"
  
  # Learning (slower learning rate to stabilize environment)
  actor_lr: 1.0e-4
  critic_lr: 5.0e-4
  use_lr_decay: true
  
  # PPO
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  n_epochs: 10
  batch_size: 64
  entropy_coef: 0.02  # Higher entropy for more exploration/variety
