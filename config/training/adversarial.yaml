
# Adversarial Training Configuration
# ==================================
# Defines hyperparameters for:
# - Training loop
# - Trader agent (PPO)
# - Adversary agent (PPO)

# Main Training Loop
# Mathematisch sinnvoll: 500 Iters = 53x Datendurchlauf (19260 Samples)
# Adversary startet bei 100 → Trader hat genug Warm-up
training:
  n_iterations: 500
  adversary_start_iteration: 999999  # Deaktiviert - Training OHNE Adversary
  steps_per_iteration: 1024
  save_frequency: 25
  log_frequency: 5
  checkpoint_dir: "data/models/adversarial"
  adversary_strength: 0.0  # Deaktiviert

# Trader Agent Configuration
# FIXES:
#   - entropy_coef: 0.01 → 0.03  (verhindert Policy-Kollaps nach Iter ~11)
#   - actor_lr: 3e-4 → 1e-4      (langsameres Lernen = stabilere Policy)
#   - target_kl: 0.01 → 0.015    (weniger aggressives Early Stopping)
#   - lr_decay_gamma: 0.99 → 0.995 (LR fällt langsamer ab)
trader:
  # Network Architecture
  hidden_dim: 128
  use_recurrent: true
  rnn_type: "GRU"
  rnn_layers: 1
  dropout: 0.1
  use_layer_norm: true
  
  # Learning
  actor_lr: 1.0e-4
  critic_lr: 5.0e-4
  use_lr_decay: true
  lr_decay_gamma: 0.995
  
  # PPO - OPTIMIERT FÜR GESCHWINDIGKEIT
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  n_epochs: 5           # REDUZIERT
  batch_size: 128       # ERHÖHT
  entropy_coef: 0.50    # ERHÖHT: für gleichmäßige Action-Verteilung
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.015      # war 0.01 → etwas lockerer für frühes Training

# Adversary Agent Configuration
adversary:
  # Network Architecture
  hidden_dim: 128
  use_recurrent: true
  rnn_type: "GRU"
  
  # Learning (langsamer als Trader für Stabilität)
  actor_lr: 5.0e-5      # war 1e-4
  critic_lr: 2.0e-4     # war 5e-4
  use_lr_decay: true
  
  # PPO
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  n_epochs: 10
  batch_size: 64
  entropy_coef: 0.05    # war 0.02 → Adversary braucht mehr Exploration
