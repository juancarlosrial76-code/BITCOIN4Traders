{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {"provenance": [], "gpuType": "T4"},
  "kernelspec": {"name": "python3", "display_name": "Python 3"},
  "language_info": {"name": "python"},
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab 3/3 — PPO Adversarial Training (GPU)\n",
    "\n",
    "**Rolle:** KI-Labor — Deep Reinforcement Learning mit GPU\n",
    "\n",
    "**Dieses Notebook tut NUR:**\n",
    "- Vorbereitete Features von Drive laden (von Notebook 1)\n",
    "- PPO Trader + Adversary auf GPU trainieren\n",
    "- Checkpoint alle N Iterationen auf Drive sichern\n",
    "- Bestes Modell fuer den Local Master bereitstellen\n",
    "\n",
    "**GPU PFLICHT! Wähle: Runtime > Change runtime type > T4 GPU**\n",
    "\n",
    "Warum GPU hier gut genutzt wird:\n",
    "- GRU-Backpropagation = parallelisierbare Matrix-Multiplikation\n",
    "- batch_size=256 auf GPU: 10x schneller als CPU\n",
    "- Der RAM bleibt stabil weil nur vorberechnete Features geladen werden\n",
    "\n",
    "---\n",
    "**Voraussetzung:** Notebook 1 muss vorher gelaufen sein\n",
    "(Features auf Drive: `MyDrive/BITCOIN4Traders/processed/train_feat.parquet`)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 1: GPU pruefen"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        'KEINE GPU!\\n'\n",
    "        'Gehe zu: Runtime > Change runtime type > Hardware accelerator: GPU (T4)\\n'\n",
    "        'Dann diese Zelle erneut ausfuehren.'\n",
    "    )\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "gpu    = torch.cuda.get_device_name(0)\n",
    "vram   = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'GPU:   {gpu}')\n",
    "print(f'VRAM:  {vram:.1f} GB')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA:    {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 2: Repo & Dependencies"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "PROJECT_DIR = '/content/BITCOIN4Traders'\n",
    "REPO_URL    = 'https://github.com/juancarlosrial76-code/BITCOIN4Traders.git'\n",
    "\n",
    "if os.path.exists(PROJECT_DIR) and not os.path.exists(f'{PROJECT_DIR}/.git'):\n",
    "    shutil.rmtree(PROJECT_DIR)\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR} --quiet\n",
    "    print('Repo geklont.')\n",
    "else:\n",
    "    !git -C {PROJECT_DIR} pull --quiet\n",
    "    print('Repo aktualisiert.')\n",
    "\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Torch ist in Colab schon vorinstalliert - nur Extras noetig\n",
    "!pip install -q loguru pyarrow pandas numpy scikit-learn pyyaml python-dotenv tqdm gymnasium\n",
    "print('Dependencies bereit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 3: Drive mounten & Konfiguration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_DIR   = '/content/drive/MyDrive/BITCOIN4Traders'\n",
    "DRIVE_PROC  = f'{DRIVE_DIR}/processed'\n",
    "DRIVE_MODEL = f'{DRIVE_DIR}/models'\n",
    "\n",
    "import os\n",
    "os.makedirs(DRIVE_MODEL, exist_ok=True)\n",
    "\n",
    "# ===== TRAINING-EINSTELLUNGEN =====\n",
    "# GPU-optimierte Werte (groessere Batches als CPU)\n",
    "N_ITERATIONS    = 500     # Anzahl PPO-Iterationen\n",
    "STEPS_PER_ITER  = 2048    # Schritte pro Iteration (Trajectory-Laenge)\n",
    "BATCH_SIZE      = 256     # Groessere Batches auf GPU (CPU: 64)\n",
    "HIDDEN_DIM      = 256     # Groesseres Netz auf GPU (CPU: 128)\n",
    "SAVE_EVERY      = 25      # Drive-Sync alle N Iterationen\n",
    "RESUME_FROM     = None    # z.B. 'checkpoint_iter_100' fuer Weitertraining\n",
    "\n",
    "print(f'Device:        {DEVICE}')\n",
    "print(f'Iterationen:   {N_ITERATIONS}')\n",
    "print(f'Batch-Size:    {BATCH_SIZE}  (GPU-optimiert)')\n",
    "print(f'Hidden-Dim:    {HIDDEN_DIM} (GPU-optimiert)')\n",
    "print(f'Drive-Sync:    alle {SAVE_EVERY} Iterationen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 4: Features von Drive laden"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "sys.path.insert(0, PROJECT_DIR)\n",
    "sys.path.insert(0, f'{PROJECT_DIR}/src')\n",
    "\n",
    "proc = Path(DRIVE_PROC)\n",
    "\n",
    "# Pruefen ob Notebook 1 gelaufen ist\n",
    "required = ['train_price.parquet', 'train_feat.parquet',\n",
    "            'val_price.parquet',   'val_feat.parquet']\n",
    "missing = [f for f in required if not (proc / f).exists()]\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f'Fehlende Dateien auf Drive: {missing}\\n'\n",
    "        f'Bitte zuerst Colab_1_Daten.ipynb ausfuehren!'\n",
    "    )\n",
    "\n",
    "logger.info('Lade vorbereitete Features von Drive...')\n",
    "\n",
    "# float32 laden - bereits von Notebook 1 konvertiert\n",
    "train_price = pd.read_parquet(proc / 'train_price.parquet')\n",
    "train_feat  = pd.read_parquet(proc / 'train_feat.parquet')\n",
    "val_price   = pd.read_parquet(proc / 'val_price.parquet')\n",
    "val_feat    = pd.read_parquet(proc / 'val_feat.parquet')\n",
    "\n",
    "logger.success(f'Train: {len(train_price):,} Bars | Val: {len(val_price):,} Bars')\n",
    "logger.success(f'Features: {train_feat.shape[1]} Spalten')\n",
    "\n",
    "# RAM-Status\n",
    "import psutil\n",
    "ram_used = psutil.virtual_memory().used / 1024**3\n",
    "ram_total = psutil.virtual_memory().total / 1024**3\n",
    "print(f'\\nRAM nach Laden: {ram_used:.1f}/{ram_total:.1f} GB')\n",
    "print(f'GPU VRAM:       {torch.cuda.memory_allocated()/1e9:.2f}/{vram:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 5: Environment erstellen"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.config_integrated_env import ConfigIntegratedTradingEnv\n",
    "from environment.config_system import EnvironmentConfig, load_environment_config_from_yaml\n",
    "\n",
    "cfg_path = Path('config/environment/realistic_env.yaml')\n",
    "if cfg_path.exists():\n",
    "    env_config = load_environment_config_from_yaml(str(cfg_path))\n",
    "    logger.info('Environment-Config aus YAML geladen')\n",
    "else:\n",
    "    env_config = EnvironmentConfig()\n",
    "    logger.warning('Standard-Config (YAML nicht gefunden)')\n",
    "\n",
    "env = ConfigIntegratedTradingEnv(train_price, train_feat, env_config)\n",
    "\n",
    "STATE_DIM  = env.observation_space.shape[0]\n",
    "N_ACTIONS  = env.action_space.n\n",
    "\n",
    "logger.success(f'Environment: State={STATE_DIM} | Actions={N_ACTIONS}')\n",
    "print(f'Observation Space: {env.observation_space.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 6: PPO Trainer erstellen (GPU-optimiert)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ppo_agent import PPOConfig\n",
    "from training.adversarial_trainer import AdversarialTrainer, AdversarialConfig\n",
    "\n",
    "# ── Trader: groesseres Netz auf GPU ─────────────────────────────────\n",
    "trader_cfg = PPOConfig(\n",
    "    state_dim=STATE_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,     # 256 auf GPU (128 auf CPU)\n",
    "    n_actions=N_ACTIONS,\n",
    "    actor_lr=1e-4,             # Reduziert (war 3e-4 -> Entropy-Kollaps)\n",
    "    critic_lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10,\n",
    "    batch_size=BATCH_SIZE,     # 256 auf GPU\n",
    "    use_recurrent=True,\n",
    "    rnn_type='GRU',\n",
    "    entropy_coef=0.08,         # Verhindert Entropy-Kollaps\n",
    "    value_loss_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=0.015,\n",
    ")\n",
    "\n",
    "# ── Adversary: etwas kleiner (CPU kann Adversary-Teil uebernehmen) ──\n",
    "adversary_cfg = PPOConfig(\n",
    "    state_dim=STATE_DIM,\n",
    "    hidden_dim=128,\n",
    "    n_actions=N_ACTIONS,\n",
    "    actor_lr=5e-5,\n",
    "    critic_lr=1e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_recurrent=True,\n",
    "    rnn_type='GRU',\n",
    "    entropy_coef=0.05,\n",
    ")\n",
    "\n",
    "training_cfg = AdversarialConfig(\n",
    "    n_iterations=N_ITERATIONS,\n",
    "    steps_per_iteration=STEPS_PER_ITER,\n",
    "    trader_config=trader_cfg,\n",
    "    adversary_config=adversary_cfg,\n",
    "    adversary_start_iteration=100,\n",
    "    adversary_strength=0.1,\n",
    "    save_frequency=SAVE_EVERY,\n",
    "    log_frequency=10,\n",
    "    checkpoint_dir='data/models/adversarial',\n",
    ")\n",
    "\n",
    "trainer = AdversarialTrainer(env, training_cfg, device=DEVICE)\n",
    "\n",
    "logger.success(f'Trainer auf {DEVICE} erstellt')\n",
    "print(f'Trader-Parameter: {sum(p.numel() for p in trainer.trader.actor.parameters()):,}')\n",
    "print(f'VRAM nach Modell: {torch.cuda.memory_allocated()/1e9:.3f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 7: [Optional] Von Checkpoint weitermachen"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob\n",
    "\n",
    "os.makedirs('data/models/adversarial', exist_ok=True)\n",
    "\n",
    "# Checkpoints von Drive nach lokal kopieren\n",
    "drive_ckpts = sorted(glob.glob(f'{DRIVE_MODEL}/*.pth'))\n",
    "\n",
    "if drive_ckpts:\n",
    "    for cp in drive_ckpts:\n",
    "        shutil.copy2(cp, 'data/models/adversarial/')\n",
    "    print(f'{len(drive_ckpts)} Checkpoint(s) von Drive geladen')\n",
    "\n",
    "    # Letzten Checkpoint laden\n",
    "    main_ckpts = [c for c in drive_ckpts if '_trader' not in c and '_adversary' not in c]\n",
    "    if main_ckpts and RESUME_FROM is None:\n",
    "        latest = sorted(main_ckpts)[-1]\n",
    "        local  = f'data/models/adversarial/{os.path.basename(latest)}'\n",
    "        try:\n",
    "            trainer.load_checkpoint(local)\n",
    "            logger.success(f'Weitertraining ab: {os.path.basename(latest)}')\n",
    "        except Exception as e:\n",
    "            logger.warning(f'Checkpoint-Laden fehlgeschlagen: {e}')\n",
    "else:\n",
    "    logger.info('Kein Checkpoint -> Training startet von Anfang an')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 8: Auto-Sync zu Drive einrichten\n",
    "\n",
    "Sichert Checkpoints alle N Iterationen. Verhindert Verlust bei Colab-Timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, shutil, os\n",
    "\n",
    "def sync_to_drive():\n",
    "    \"\"\"Kopiert alle lokalen Checkpoints auf Google Drive.\"\"\"\n",
    "    local  = 'data/models/adversarial'\n",
    "    remote = DRIVE_MODEL\n",
    "    copied = 0\n",
    "    for cp in glob.glob(f'{local}/*.pth'):\n",
    "        dst = os.path.join(remote, os.path.basename(cp))\n",
    "        shutil.copy2(cp, dst)\n",
    "        copied += 1\n",
    "    if copied:\n",
    "        print(f'Drive sync: {copied} Datei(en) gesichert')\n",
    "\n",
    "# Sofort testen\n",
    "sync_to_drive()\n",
    "print('Auto-Sync bereit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 9: TRAINING STARTEN\n",
    "\n",
    "RAM-Verbrauch sollte STABIL bleiben:\n",
    "- history begrenzt auf 200 Eintraege (memory_management.yaml)\n",
    "- adversary_states nach jedem Training geloescht\n",
    "- torch.cuda.empty_cache() alle 10 Iterationen\n",
    "- gc.collect() integriert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import psutil\n",
    "\n",
    "def ram_status():\n",
    "    ram_used  = psutil.virtual_memory().used / 1024**3\n",
    "    ram_total = psutil.virtual_memory().total / 1024**3\n",
    "    vram_used = torch.cuda.memory_allocated() / 1e9\n",
    "    return f'RAM {ram_used:.1f}/{ram_total:.1f}GB | VRAM {vram_used:.2f}GB'\n",
    "\n",
    "logger.info('=' * 70)\n",
    "logger.info('PPO ADVERSARIAL TRAINING GESTARTET')\n",
    "logger.info(f'Device: {DEVICE} | Batch: {BATCH_SIZE} | Hidden: {HIDDEN_DIM}')\n",
    "logger.info(f'Start-RAM: {ram_status()}')\n",
    "logger.info('=' * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "_done = False\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    _done = True\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning('Unterbrochen - speichere...')\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e).lower():\n",
    "        logger.error(f'GPU OOM! {e}')\n",
    "        logger.info('Tipp: BATCH_SIZE auf 128 reduzieren und neu starten')\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "finally:\n",
    "    elapsed = (time.time() - start_time) / 3600\n",
    "    logger.info(f'Dauer: {elapsed:.1f}h | End-RAM: {ram_status()}')\n",
    "    sync_to_drive()\n",
    "    logger.success('Checkpoints auf Drive gesichert')\n",
    "    if _done:\n",
    "        logger.success('Training vollstaendig abgeschlossen!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Schritt 10: Modell evaluieren"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation auf Validation-Set\n",
    "from environment.config_integrated_env import ConfigIntegratedTradingEnv\n",
    "\n",
    "val_env = ConfigIntegratedTradingEnv(val_price, val_feat, env_config)\n",
    "val_trainer = AdversarialTrainer(val_env, training_cfg, device=DEVICE)\n",
    "\n",
    "# Bestes Modell laden\n",
    "best_path = 'data/models/adversarial/best_model_trader.pth'\n",
    "if os.path.exists(best_path):\n",
    "    val_trainer.trader.load(best_path)\n",
    "    logger.success(f'Bestes Modell geladen: {best_path}')\n",
    "\n",
    "try:\n",
    "    metrics = trainer.evaluate(n_episodes=50)\n",
    "    print('\\n=== Evaluationsergebnisse (Validation-Set) ===')\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f'  {k}: {v:.4f}')\n",
    "\n",
    "    # Modell auch als ppo_best.pt speichern (fuer run.py)\n",
    "    import torch\n",
    "    torch.save(trainer.trader.state_dict(), 'data/models/ppo_best.pt')\n",
    "    shutil.copy2('data/models/ppo_best.pt', f'{DRIVE_MODEL}/ppo_best.pt')\n",
    "    logger.success('Modell als ppo_best.pt gespeichert (fuer Live-Trading mit run.py)')\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f'Evaluation fehlgeschlagen: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fertig!\n",
    "\n",
    "Das trainierte PPO-Modell liegt jetzt auf:\n",
    "- `MyDrive/BITCOIN4Traders/models/best_model_trader.pth`\n",
    "- `MyDrive/BITCOIN4Traders/models/ppo_best.pt`\n",
    "\n",
    "**Naechste Schritte:**\n",
    "1. `ppo_best.pt` in dein Repo kopieren: `data/models/ppo_best.pt`\n",
    "2. In `run.py` den StubAgent durch den echten PPOAgent ersetzen\n",
    "3. `pm2 start ecosystem.config.js` auf dem Linux Local Master"
   ]
  }
 ]
}
