{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "None"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "None"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab 1/3 — Daten-Aufbereitung\n",
    "\n",
    "**Rolle in der 3-Saeulen-Architektur:** KI-Labor (Daten-Vorbereitung)\n",
    "\n",
    "**Dieses Notebook tut NUR:**\n",
    "- BTC/USDT OHLCV-Daten von Binance oder Yahoo Finance laden\n",
    "- Features berechnen (FeatureEngine)\n",
    "- Scaler fitten und speichern\n",
    "- Alles auf Google Drive sichern fuer Notebook 2 und 3\n",
    "\n",
    "**Kein GPU noetig! Wähle bei Runtime: `None` (CPU)**\n",
    "\n",
    "---\n",
    "**Warum 3 getrennte Notebooks?**\n",
    "Ein einzelnes Notebook das alles macht (Daten + Evolution + PPO) verbraucht nach\n",
    "~1h den gesamten RAM (12 GB). Die Daten bleiben im RAM waehrend das Training laeuft.\n",
    "Mit 3 Notebooks wird jede Aufgabe in einer frischen Session gestartet - kein Altlast-RAM.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 1: Repo klonen & Dependencies installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "PROJECT_DIR = '/content/BITCOIN4Traders'\n",
    "REPO_URL    = 'https://github.com/juancarlosrial76-code/BITCOIN4Traders.git'\n",
    "\n",
    "if os.path.exists(PROJECT_DIR) and not os.path.exists(f'{PROJECT_DIR}/.git'):\n",
    "    shutil.rmtree(PROJECT_DIR)\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR} --quiet\n",
    "    print('Repo geklont.')\n",
    "else:\n",
    "    !git -C {PROJECT_DIR} pull --quiet\n",
    "    print('Repo aktualisiert.')\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f'Verzeichnis: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Nur Daten-Dependencies - kein torch, kein gymnasium\n",
    "# Schneller + weniger RAM als full install\n",
    "!pip install -q ccxt loguru pyarrow pandas numpy ta yfinance numba joblib pyyaml scikit-learn python-dotenv tqdm\n",
    "print('Dependencies installiert.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 2: Google Drive mounten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_DIR   = '/content/drive/MyDrive/BITCOIN4Traders'\n",
    "DRIVE_DATA  = f'{DRIVE_DIR}/data'\n",
    "DRIVE_PROC  = f'{DRIVE_DIR}/processed'\n",
    "\n",
    "import os\n",
    "for d in [DRIVE_DIR, DRIVE_DATA, DRIVE_PROC]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f'Drive bereit: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 3: Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ===== EINSTELLUNGEN =====\n",
    "SYMBOL      = 'BTC/USDT'\n",
    "TIMEFRAME   = '1h'\n",
    "END_DATE    = None            # None = bis heute\n",
    "\n",
    "# Datentyp: float32 spart 50% RAM vs float64\n",
    "DTYPE       = 'float32'\n",
    "\n",
    "# Max Candles in RAM halten (Colab: 12 GB Limit)\n",
    "MAX_CANDLES = 50_000  # ~5.7 Jahre a 1h; Repo hat 74.650 Bars seit 2017\n",
    "\n",
    "# Yahoo Finance erlaubt fuer 1h-Daten maximal 729 Tage zurueck.\n",
    "# Wir rechnen das automatisch aus - kein manuelles Anpassen noetig.\n",
    "YF_MAX_DAYS = 729\n",
    "YF_START    = (datetime.utcnow() - timedelta(days=YF_MAX_DAYS)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Binance/KuCoin koennen weiter zurueck (seit 2020), aber Colab-IPs\n",
    "# werden von Binance US geblockt (451). KuCoin ist der zuverlaessigste Fallback.\n",
    "CCXT_START  = '2022-01-01'  # 3 Jahre - genuegt fuer robustes Training\n",
    "\n",
    "print(f'Symbol:          {SYMBOL}')\n",
    "print(f'CCXT Start:      {CCXT_START}')\n",
    "print(f'YF Start (auto): {YF_START}  (max {YF_MAX_DAYS} Tage fuer 1h-Daten)')\n",
    "print(f'Max Bars:        {MAX_CANDLES:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 4: Daten laden (Binance oder Yahoo Finance Fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, gc, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "logger.remove()  # Remove default stderr handler\n",
    "logger.add(sys.stdout, format=\"{time:HH:mm:ss} | {level} | {message}\", level=\"DEBUG\", colorize=False)\n",
    "\n",
    "sys.path.insert(0, '/content/BITCOIN4Traders')\n",
    "sys.path.insert(0, '/content/BITCOIN4Traders/src')\n",
    "\n",
    "DRIVE_CACHE = Path(DRIVE_DATA) / 'BTC_USDT_1h_raw.parquet'\n",
    "# Direkt aus dem Repo (von Linux Local Master hochgeladen)\n",
    "REPO_CACHE  = Path(PROJECT_DIR) / 'data/cache/BTC_USDT_1h_binance.parquet'\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# Hilfsfunktionen\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "def ccxt_to_df(ohlcv_list, dtype='float32'):\n",
    "    df = pd.DataFrame(ohlcv_list, columns=['timestamp','open','high','low','close','volume'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df = df.set_index('timestamp').sort_index()\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "    return df.astype(dtype)\n",
    "\n",
    "def fetch_ccxt(exchange_id, symbol, timeframe, start_date, dtype='float32'):\n",
    "    import ccxt\n",
    "    ex = getattr(ccxt, exchange_id)({'enableRateLimit': True})\n",
    "    since_ms = ex.parse8601(f'{start_date}T00:00:00Z')\n",
    "    all_ohlcv, limit = [], 1000\n",
    "    logger.info(f'Lade von {exchange_id} ({symbol} {timeframe} ab {start_date})...')\n",
    "    while True:\n",
    "        batch = ex.fetch_ohlcv(symbol, timeframe, since=since_ms, limit=limit)\n",
    "        if not batch: break\n",
    "        all_ohlcv.extend(batch)\n",
    "        since_ms = batch[-1][0] + 1\n",
    "        if len(batch) < limit: break\n",
    "        time.sleep(0.25)\n",
    "    if not all_ohlcv:\n",
    "        raise ValueError(f'Keine Daten von {exchange_id}')\n",
    "    df = ccxt_to_df(all_ohlcv, dtype)\n",
    "    logger.success(f'{exchange_id}: {len(df):,} Bars')\n",
    "    return df\n",
    "\n",
    "def fetch_yfinance(symbol_yf, start_date, dtype='float32'):\n",
    "    import yfinance as yf\n",
    "    logger.info(f'Lade {symbol_yf} von Yahoo Finance (ab {start_date})...')\n",
    "    df = yf.download(symbol_yf, start=start_date, interval='1h',\n",
    "                     progress=False, auto_adjust=True)\n",
    "    if df.empty:\n",
    "        raise ValueError(f'Keine Daten von Yahoo Finance')\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "    df = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "    if hasattr(df.index, 'tz') and df.index.tz is not None:\n",
    "        df.index = df.index.tz_localize(None)\n",
    "    df.index.name = 'timestamp'\n",
    "    df = df.astype(dtype)\n",
    "    logger.success(f'Yahoo Finance: {len(df):,} Bars')\n",
    "    return df\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# Reihenfolge: Drive-Cache → Repo (GitHub) → Exchanges → Yahoo Finance\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "price_data = None\n",
    "MIN_BARS = 2_000\n",
    "\n",
    "# Quelle 1: Drive-Cache (schnellste Option wenn schon vorhanden)\n",
    "if DRIVE_CACHE.exists():\n",
    "    price_data = pd.read_parquet(DRIVE_CACHE)\n",
    "    logger.success(f'Drive-Cache: {len(price_data):,} Bars')\n",
    "\n",
    "# Quelle 2: Repo-Cache (von Linux Local Master gepusht - 74.650 Bars seit 2017!)\n",
    "if price_data is None or len(price_data) < MIN_BARS:\n",
    "    if REPO_CACHE.exists():\n",
    "        price_data = pd.read_parquet(REPO_CACHE)\n",
    "        logger.success(f'GitHub Repo-Cache: {len(price_data):,} Bars (2017-heute)')\n",
    "        # Auf Drive kopieren fuer zukuenftige Sessions\n",
    "        price_data.to_parquet(DRIVE_CACHE, engine='pyarrow', compression='snappy')\n",
    "        logger.info(f'Auf Drive gecacht: {DRIVE_CACHE}')\n",
    "\n",
    "# Quelle 3: Exchanges (falls Repo-Cache fehlt)\n",
    "if price_data is None or len(price_data) < MIN_BARS:\n",
    "    for ex_id in ['kucoin', 'bybit', 'okx', 'kraken']:\n",
    "        try:\n",
    "            df = fetch_ccxt(ex_id, 'BTC/USDT', TIMEFRAME, CCXT_START, DTYPE)\n",
    "            if len(df) >= MIN_BARS:\n",
    "                price_data = df\n",
    "                break\n",
    "            logger.warning(f'{ex_id}: nur {len(df):,} Bars')\n",
    "        except Exception as e:\n",
    "            logger.warning(f'{ex_id}: {e}')\n",
    "\n",
    "# Quelle 4: Yahoo Finance (letzter Ausweg, max 729 Tage)\n",
    "if price_data is None or len(price_data) < MIN_BARS:\n",
    "    try:\n",
    "        price_data = fetch_yfinance('BTC-USD', YF_START, DTYPE)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Yahoo Finance: {e}')\n",
    "\n",
    "if price_data is None:\n",
    "    raise RuntimeError('Alle Datenquellen fehlgeschlagen!')\n",
    "\n",
    "# NaN und Nullwerte entfernen\n",
    "price_data = price_data.replace(0, np.nan).dropna()\n",
    "\n",
    "# Auf MAX_CANDLES begrenzen (neueste Bars)\n",
    "if len(price_data) > MAX_CANDLES:\n",
    "    price_data = price_data.iloc[-MAX_CANDLES:]\n",
    "\n",
    "mem_mb = price_data.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'\\nDaten: {len(price_data):,} Bars | RAM: {mem_mb:.1f} MB | dtype: {price_data.dtypes[0]}')\n",
    "print(f'Zeitraum: {price_data.index[0]} bis {price_data.index[-1]}')\n",
    "print(price_data.tail(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 5: Features berechnen & Scaler speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from features.feature_engine import FeatureEngine, FeatureConfig\n",
    "\n",
    "PROC_DIR = Path(DRIVE_PROC)\n",
    "n_total  = len(price_data)\n",
    "\n",
    "# ── Sicherheitscheck: genuegend Daten vorhanden? ─────────────────────\n",
    "# FeatureEngine braucht mind. 20 Bars fuer Rolling-Features (NaN-Warmup).\n",
    "# Val/Test muessen nach Warmup noch mind. 50 nutzbare Bars haben.\n",
    "# Minimum gesamt: 20 (Warmup) + 500 (Train) + 50 (Val) + 50 (Test) = 620\n",
    "MIN_TOTAL = 620\n",
    "if n_total < MIN_TOTAL:\n",
    "    raise RuntimeError(\n",
    "        f'Zu wenig Daten: {n_total} Bars (Minimum: {MIN_TOTAL}).\\n'\n",
    "        f'Bitte Cache loeschen und Zelle 4 erneut ausfuehren - '\n",
    "        f'KuCoin sollte ~17.000 Bars liefern.'\n",
    "    )\n",
    "\n",
    "# ── Split: 70% Train, 15% Val, 15% Test ─────────────────────────────\n",
    "# min_valid_rows wird proportional zur Menge gesetzt:\n",
    "# Train bekommt 500, Val/Test bekommen 10% der jeweiligen Groesse (mind. 50).\n",
    "train_end = int(n_total * 0.70)\n",
    "val_end   = int(n_total * 0.85)\n",
    "\n",
    "train_raw = price_data.iloc[:train_end]\n",
    "val_raw   = price_data.iloc[train_end:val_end]\n",
    "test_raw  = price_data.iloc[val_end:]\n",
    "\n",
    "# min_valid_rows: Val und Test duerfen kleiner sein als Train\n",
    "# (sie brauchen kein Fit - nur Transform)\n",
    "min_train = min(500, max(50, int(len(train_raw) * 0.80)))\n",
    "min_valtest = max(20, int(min(len(val_raw), len(test_raw)) * 0.80))\n",
    "\n",
    "logger.info(f'Gesamt: {n_total:,} Bars')\n",
    "logger.info(f'Split:  Train={len(train_raw):,} | Val={len(val_raw):,} | Test={len(test_raw):,}')\n",
    "logger.info(f'min_valid_rows: Train={min_train} | Val/Test={min_valtest}')\n",
    "\n",
    "# ── Feature Engineering ─────────────────────────────────────────────\n",
    "# Gemeinsame Basis-Config\n",
    "base_cfg = dict(\n",
    "    volatility_window=20,\n",
    "    ou_window=20,\n",
    "    rolling_mean_window=20,\n",
    "    use_log_returns=True,\n",
    "    scaler_type='standard',\n",
    "    save_scaler=True,\n",
    "    scaler_path=PROC_DIR,\n",
    "    dropna_strategy='rolling',\n",
    ")\n",
    "\n",
    "# Fit NUR auf Trainingsdaten (kein Data Leakage)\n",
    "engine = FeatureEngine(FeatureConfig(**base_cfg, min_valid_rows=min_train))\n",
    "logger.info('Fit FeatureEngine auf Trainingsdaten...')\n",
    "train_feat = engine.fit_transform(train_raw)\n",
    "\n",
    "# Val/Test: eigene Engine-Instanz mit niedrigerem min_valid_rows\n",
    "# Scaler + train_stats vom gefitteten engine uebernehmen (kein Leakage)\n",
    "engine_vt = FeatureEngine(FeatureConfig(**base_cfg, min_valid_rows=min_valtest))\n",
    "engine_vt.scaler      = engine.scaler       # Scaler vom Train-Set uebernehmen\n",
    "engine_vt.is_fitted   = True\n",
    "engine_vt.train_stats = engine.train_stats\n",
    "val_feat  = engine_vt.transform(val_raw)\n",
    "test_feat = engine_vt.transform(test_raw)\n",
    "\n",
    "# Indizes angleichen (Rolling-Features entfernen die ersten 20 Bars)\n",
    "idx_train = train_raw.index.intersection(train_feat.index)\n",
    "idx_val   = val_raw.index.intersection(val_feat.index)\n",
    "idx_test  = test_raw.index.intersection(test_feat.index)\n",
    "\n",
    "logger.success(f'Features: {train_feat.shape[1]} Spalten')\n",
    "logger.success(f'Nutzbare Bars: Train={len(idx_train):,} | Val={len(idx_val):,} | Test={len(idx_test):,}')\n",
    "\n",
    "# ── Auf Drive speichern (komprimiert float32) ───────────────────────\n",
    "def save_split(price, feat, idx, name):\n",
    "    p_path = PROC_DIR / f'{name}_price.parquet'\n",
    "    f_path = PROC_DIR / f'{name}_feat.parquet'\n",
    "    price.loc[idx].astype('float32').to_parquet(p_path, compression='snappy')\n",
    "    feat.loc[idx].astype('float32').to_parquet(f_path, compression='snappy')\n",
    "    mb = (p_path.stat().st_size + f_path.stat().st_size) / 1024**2\n",
    "    logger.success(f'Gespeichert: {name} ({len(idx):,} Bars, {mb:.1f} MB)')\n",
    "\n",
    "save_split(train_raw, train_feat, idx_train, 'train')\n",
    "save_split(val_raw,   val_feat,   idx_val,   'val')\n",
    "save_split(test_raw,  test_feat,  idx_test,  'test')\n",
    "\n",
    "# ── RAM freigeben ───────────────────────────────────────────────────\n",
    "del train_feat, val_feat, test_feat, train_raw, val_raw, test_raw, price_data\n",
    "gc.collect()\n",
    "\n",
    "print('\\nDaten-Aufbereitung abgeschlossen!')\n",
    "print(f'Dateien auf Drive: {PROC_DIR}')\n",
    "print('Weiter mit: Colab_2_Evolution.ipynb oder Colab_3_PPO_Training.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 6: Ergebnis pruefen\n",
    "\n",
    "Zeigt alle gespeicherten Dateien auf Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print('=== Gespeicherte Dateien auf Drive ===')\n",
    "total = 0\n",
    "for f in sorted(Path(DRIVE_PROC).iterdir()):\n",
    "    mb = f.stat().st_size / 1024**2\n",
    "    total += mb\n",
    "    print(f'  {f.name:35s}  {mb:.1f} MB')\n",
    "print(f'\\nGesamt: {total:.1f} MB')\n",
    "print('\\nNotebook 1 fertig. Starte jetzt Notebook 2 oder 3.')"
   ]
  }
 ]
}