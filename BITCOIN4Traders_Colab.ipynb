{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BITCOIN4Traders - Google Colab Training\n",
    "\n",
    "**Anleitung:**\n",
    "1. Gehe zu `Runtime > Change runtime type` und wähle **GPU (T4)**\n",
    "2. Führe alle Zellen der Reihe nach aus\n",
    "3. Das Modell wird automatisch auf Google Drive gespeichert\n",
    "4. Bei Unterbrechung: Zelle 1-4 erneut ausführen, dann Resume-Zelle\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 0b: Error-Handler & Hilferuf (Auto-Repair)\n",
    "> **Automatisch eingefügt.** Sendet Fehler an Linux-PC für Selbstheilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# AUTO-REPAIR: Error-Handler & Hilferuf zum Linux-PC\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# Sendet Fehler automatisch an den Linux-Master.\n",
    "# Kanal 1: HTTP (schnell, direkter Weg)\n",
    "# Kanal 2: Google Drive (Fallback, kein offener Port nötig)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── Konfiguration ───────────────────────────────────────────────\n",
    "NOTEBOOK_ID     = 'BITCOIN4Traders_Colab_v1'\n",
    "LINUX_IP        = ''           # z.B. '192.168.1.100' oder ngrok-URL\n",
    "LINUX_PORT      = 5001\n",
    "LINUX_API_TOKEN = ''           # Von: python3 infrastructure/monitor/listener.py setup\n",
    "DRIVE_FOLDER_ID = ''           # Google Drive Ordner-ID (Fallback)\n",
    "\n",
    "# ─── Hilferuf-Funktion ───────────────────────────────────────────\n",
    "def send_help(error_msg: str, error_type: str = 'Exception',\n",
    "              extra: dict = None):\n",
    "    \"\"\"Sendet Fehlerbericht an Linux-PC via HTTP oder Drive.\"\"\"\n",
    "    payload = {\n",
    "        'notebook_id':   NOTEBOOK_ID,\n",
    "        'error_type':    error_type,\n",
    "        'error_message': str(error_msg)[:500],\n",
    "        'stacktrace':    traceback.format_exc()[:2000],\n",
    "        'timestamp':     datetime.now().isoformat(),\n",
    "        'extra':         extra or {},\n",
    "    }\n",
    "\n",
    "    # Kanal 1: HTTP\n",
    "    if LINUX_IP and LINUX_API_TOKEN:\n",
    "        try:\n",
    "            r = requests.post(\n",
    "                f'http://{LINUX_IP}:{LINUX_PORT}/report_error',\n",
    "                json=payload,\n",
    "                headers={'X-API-Token': LINUX_API_TOKEN},\n",
    "                timeout=10\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                print(f'✅ Hilferuf empfangen (HTTP): {r.json()}')\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(f'⚠️ HTTP fehlgeschlagen ({e}) - versuche Drive...')\n",
    "\n",
    "    # Kanal 2: Google Drive (Fallback)\n",
    "    _send_help_via_drive(payload)\n",
    "\n",
    "\n",
    "def _send_help_via_drive(payload: dict):\n",
    "    \"\"\"Schreibt Fehlerbericht in Google Drive (Fallback).\"\"\"\n",
    "    if not DRIVE_FOLDER_ID:\n",
    "        print('⚠️ DRIVE_FOLDER_ID nicht gesetzt - Hilferuf konnte nicht gesendet werden')\n",
    "        return\n",
    "    try:\n",
    "        from google.colab import auth\n",
    "        from googleapiclient.discovery import build\n",
    "        from googleapiclient.http import MediaFileUpload\n",
    "        import tempfile\n",
    "        auth.authenticate_user()\n",
    "        from google.auth import default\n",
    "        creds, _ = default()\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "            json.dump(payload, f)\n",
    "            tmp = f.name\n",
    "        media = MediaFileUpload(tmp, mimetype='application/json')\n",
    "        service.files().create(\n",
    "            body={'name': 'colab_error_report.json', 'parents': [DRIVE_FOLDER_ID]},\n",
    "            media_body=media\n",
    "        ).execute()\n",
    "        print('✅ Hilferuf via Drive gesendet')\n",
    "    except Exception as e:\n",
    "        print(f'❌ Drive-Hilferuf fehlgeschlagen: {e}')\n",
    "\n",
    "\n",
    "# ─── Status-Update (periodisch) ──────────────────────────────────\n",
    "def report_status(step: int, reward: float, extra: dict = None):\n",
    "    \"\"\"Sendet Heartbeat mit Trainings-Fortschritt.\"\"\"\n",
    "    if not DRIVE_FOLDER_ID:\n",
    "        return\n",
    "    try:\n",
    "        status = {\n",
    "            'timestamp':    datetime.now().isoformat(),\n",
    "            'notebook_id':  NOTEBOOK_ID,\n",
    "            'training_step': step,\n",
    "            'last_reward':  round(float(reward), 4),\n",
    "            'status':       'training',\n",
    "            **(extra or {}),\n",
    "        }\n",
    "        # Lokal in Drive schreiben\n",
    "        status_path = '/content/drive/MyDrive/BITCOIN4Traders/colab_status.json'\n",
    "        with open(status_path, 'w') as f:\n",
    "            json.dump(status, f)\n",
    "    except Exception:\n",
    "        pass  # Niemals Training unterbrechen wegen Status-Update\n",
    "\n",
    "\n",
    "print('✅ Error-Handler geladen')\n",
    "print(f'   Notebook-ID: {NOTEBOOK_ID}')\n",
    "print(f'   HTTP-Kanal:  {LINUX_IP}:{LINUX_PORT} ', '✅' if LINUX_IP else '⚠️ nicht konfiguriert')\n",
    "print(f'   Drive-Kanal: ', '✅' if DRIVE_FOLDER_ID else '⚠️ nicht konfiguriert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 1: GPU prüfen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU verfügbar: {gpu_name}')\n",
    "    print(f'GPU Speicher: {gpu_mem:.1f} GB')\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print('WARNUNG: Keine GPU gefunden! Gehe zu Runtime > Change runtime type > GPU')\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "print(f'Verwende Device: {DEVICE}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 2: Google Drive mounten (für persistente Speicherung)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive mounten\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Projektordner auf Drive erstellen\n",
    "DRIVE_PROJECT_DIR = '/content/drive/MyDrive/BITCOIN4Traders'\n",
    "DRIVE_MODEL_DIR = f'{DRIVE_PROJECT_DIR}/models'\n",
    "DRIVE_DATA_DIR = f'{DRIVE_PROJECT_DIR}/data'\n",
    "DRIVE_LOG_DIR = f'{DRIVE_PROJECT_DIR}/logs'\n",
    "\n",
    "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DRIVE_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DRIVE_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Google Drive gemountet.')\n",
    "print(f'Modelle werden gespeichert in: {DRIVE_MODEL_DIR}')\n",
    "print(f'Daten werden gespeichert in: {DRIVE_DATA_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 3: Repository klonen / Projekt hochladen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nimport shutil\n\nPROJECT_DIR = '/content/BITCOIN4Traders'\nREPO_URL = 'https://github.com/juancarlosrial76-code/BITCOIN4Traders.git'\n\n# Wenn Ordner existiert aber kein gültiges Git-Repo ist -> löschen\nif os.path.exists(PROJECT_DIR) and not os.path.exists(os.path.join(PROJECT_DIR, '.git')):\n    print(f'Ungültiger Projektordner gefunden - wird gelöscht...')\n    shutil.rmtree(PROJECT_DIR)\n\nif not os.path.exists(PROJECT_DIR):\n    print('Klone Projekt von GitHub...')\n    !git clone {REPO_URL} {PROJECT_DIR}\n    print('Klon abgeschlossen!')\nelse:\n    print('Projekt vorhanden - aktualisiere...')\n    !git -C {PROJECT_DIR} pull\n    print('Aktualisiert!')\n\n# Prüfung\nsrc_ok = os.path.exists(os.path.join(PROJECT_DIR, 'src', 'data', 'ccxt_loader.py'))\nprint(f'\\nProjektstruktur OK: {src_ok}')\nif not src_ok:\n    raise RuntimeError(\"Klon fehlgeschlagen! Bitte Runtime neu starten und erneut versuchen.\")\n\nos.chdir(PROJECT_DIR)\nprint(f'Bereit: {PROJECT_DIR}')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 4: Dependencies installieren"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\nprint('Installiere Dependencies...')\n\n!pip install -q ccxt loguru pyarrow pandas numpy scipy gymnasium stable-baselines3 ta yfinance numba hmmlearn scikit-learn pyyaml pydantic python-dotenv tqdm joblib matplotlib plotly omegaconf\n\n# Sicherstellen dass ccxt wirklich installiert ist\nimport importlib\nfor pkg in ['ccxt', 'loguru', 'pyarrow', 'gymnasium', 'omegaconf']:\n    try:\n        importlib.import_module(pkg)\n        print(f'  OK: {pkg}')\n    except ImportError:\n        print(f'  FEHLT: {pkg} - installiere nochmals...')\n        import subprocess\n        subprocess.run(['pip', 'install', '-q', pkg], check=True)\n\nprint('Installation abgeschlossen!')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 5: Python-Pfad setzen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_DIR = '/content/BITCOIN4Traders'\n",
    "SRC_DIR = os.path.join(PROJECT_DIR, 'src')\n",
    "\n",
    "# Pfade hinzufügen\n",
    "for path in [PROJECT_DIR, SRC_DIR]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "# In Projektordner wechseln\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f'Arbeitsverzeichnis: {os.getcwd()}')\n",
    "print(f'Python-Pfad enthält: {SRC_DIR}')\n",
    "\n",
    "# Notwendige Verzeichnisse erstellen\n",
    "dirs = [\n",
    "    'data/cache',\n",
    "    'data/processed', \n",
    "    'data/models/adversarial',\n",
    "    'logs/training'\n",
    "]\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('Verzeichnisse erstellt.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 6: Daten von Drive laden oder herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "DRIVE_DATA_DIR = '/content/drive/MyDrive/BITCOIN4Traders/data'\n",
    "LOCAL_CACHE_DIR = '/content/BITCOIN4Traders/data/cache'\n",
    "\n",
    "# Prüfen ob gecachte Daten auf Drive vorhanden\n",
    "drive_cache_files = []\n",
    "if os.path.exists(DRIVE_DATA_DIR):\n",
    "    drive_cache_files = [f for f in os.listdir(DRIVE_DATA_DIR) if f.endswith('.parquet')]\n",
    "\n",
    "if drive_cache_files:\n",
    "    print(f'Lade gecachte Daten von Drive: {drive_cache_files}')\n",
    "    for fname in drive_cache_files:\n",
    "        src = os.path.join(DRIVE_DATA_DIR, fname)\n",
    "        dst = os.path.join(LOCAL_CACHE_DIR, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f'  Kopiert: {fname}')\n",
    "    print('Daten erfolgreich von Drive geladen!')\n",
    "else:\n",
    "    print('Keine gecachten Daten auf Drive gefunden.')\n",
    "    print('Daten werden beim Training von Binance heruntergeladen.')\n",
    "    print('(Dies dauert einige Minuten beim ersten Start)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 7: Training-Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n\n# ===== TRAINING-EINSTELLUNGEN =====\n# Diese Werte kannst du anpassen\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Datensatz\nSYMBOL = 'BTC/USDT'        # Handelspaar\nTIMEFRAME = '1h'            # Zeitrahmen: 1m, 5m, 15m, 1h, 4h, 1d\nSTART_DATE = '2022-01-01'   # Startdatum (mehr Daten = besseres Training)\nEND_DATE = None              # None = bis heute\nEXCHANGE = 'kucoin'\n\n# Training\nN_ITERATIONS = 500           # Anzahl Trainingsiterationen\nSTEPS_PER_ITER = 2048        # Schritte pro Iteration\nSAVE_FREQUENCY = 25          # Speichern alle N Iterationen (öfter als Standard)\n\n# Checkpoint (für Resume)\nRESUME_CHECKPOINT = None     # Pfad zu Checkpoint, z.B. 'data/models/adversarial/checkpoint_iter_100.pth'\n\nprint('Konfiguration:')\nprint(f'  Device:     {DEVICE}')\nprint(f'  Symbol:     {SYMBOL}')\nprint(f'  Timeframe:  {TIMEFRAME}')\nprint(f'  Start:      {START_DATE}')\nprint(f'  Iterationen: {N_ITERATIONS}')\nprint(f'  Save alle:  {SAVE_FREQUENCY} Iterationen')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 8: Daten laden & Features berechnen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom loguru import logger\nimport sys, os, importlib.util, subprocess\n\nPROJECT_DIR = '/content/BITCOIN4Traders'\nSRC_DIR = os.path.join(PROJECT_DIR, 'src')\nccxt_loader_path = os.path.join(SRC_DIR, 'data', 'ccxt_loader.py')\n\n# Logging\nlogger.remove()\nlogger.add(sys.stdout, format=\"{time:HH:mm:ss} | {level} | {message}\", level=\"INFO\")\n# Log-Datei mit Rotation: max 50 MB, max 3 Backups -> verhindert unkontrollierten Speicheranstieg\nlogger.add(\n    '/content/BITCOIN4Traders/logs/training/training.log',\n    format=\"{time:HH:mm:ss} | {level} | {message}\",\n    level=\"INFO\",\n    rotation=\"50 MB\",\n    retention=3,\n    compression=\"gz\",\n)\n\n# Absolute Pfade\ncache_dir = Path(os.path.join(PROJECT_DIR, 'data', 'cache'))\nprocessed_dir = Path(os.path.join(PROJECT_DIR, 'data', 'processed'))\ncache_dir.mkdir(parents=True, exist_ok=True)\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\ndef load_via_yfinance(symbol, start_date, end_date, cache_dir):\n    \"\"\"Lädt BTC Daten via Yahoo Finance - kein Geo-Block.\"\"\"\n    import yfinance as yf\n    # BTC/USDT -> BTC-USD für Yahoo Finance\n    yf_symbol = symbol.replace('/', '-').replace('USDT', 'USD')\n    logger.info(f'Lade {yf_symbol} von Yahoo Finance...')\n    df = yf.download(yf_symbol, start=start_date, end=end_date, interval='1h', progress=False, auto_adjust=True)\n    if df.empty:\n        raise ValueError(f'Keine Daten von Yahoo Finance für {yf_symbol}')\n    # Spalten anpassen\n    df.columns = [c.lower() for c in df.columns]\n    df = df[['open', 'high', 'low', 'close', 'volume']]\n    df.index.name = 'timestamp'\n    df.index = pd.to_datetime(df.index, utc=True).tz_localize(None)\n    # Cache speichern\n    cache_path = cache_dir / f'BTC_USDT_1h_yfinance.parquet'\n    df.to_parquet(cache_path, engine='pyarrow', compression='snappy')\n    logger.success(f'Yahoo Finance: {len(df)} Candles geladen')\n    return df\n\ndef load_via_ccxt(exchange_id, symbol, timeframe, start_date, end_date, cache_dir, processed_dir):\n    \"\"\"Lädt Daten via CCXT.\"\"\"\n    spec = importlib.util.spec_from_file_location(\"ccxt_loader\", ccxt_loader_path)\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)\n    config = mod.DataLoaderConfig(\n        exchange_id=exchange_id,\n        exchange_type='spot',\n        rate_limit_ms=200,\n        cache_dir=cache_dir,\n        processed_dir=processed_dir,\n        compression='snappy',\n    )\n    loader = mod.CCXTDataLoader(config)\n    return loader.download_and_cache(\n        symbol=symbol, timeframe=timeframe,\n        start_date=start_date, end_date=end_date,\n        force_refresh=False,\n    )\n\n# Cache prüfen\ncached_files = list(cache_dir.glob('*.parquet'))\nif cached_files:\n    logger.info(f'Lade gecachte Daten: {cached_files[0]}')\n    price_data = pd.read_parquet(cached_files[0])\n    logger.success(f'Geladen: {len(price_data)} Candles')\nelse:\n    # Versuche CCXT Exchanges ohne Geo-Block\n    exchanges_to_try = ['kucoin', 'bybit', 'okx', 'gateio']\n    price_data = None\n\n    for exchange_id in exchanges_to_try:\n        try:\n            logger.info(f'Versuche {exchange_id}...')\n            price_data = load_via_ccxt(\n                exchange_id, SYMBOL, TIMEFRAME,\n                START_DATE, END_DATE,\n                cache_dir, processed_dir\n            )\n            logger.success(f'{exchange_id}: Erfolgreich!')\n            break\n        except Exception as e:\n            logger.warning(f'{exchange_id} fehlgeschlagen: {str(e)[:80]}')\n            continue\n\n    # Fallback: Yahoo Finance\n    if price_data is None:\n        logger.info('Alle CCXT Exchanges fehlgeschlagen - nutze Yahoo Finance als Fallback...')\n        try:\n            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'yfinance'], check=True)\n            price_data = load_via_yfinance(SYMBOL, START_DATE, END_DATE, cache_dir)\n        except Exception as e:\n            raise RuntimeError(f'Alle Datenquellen fehlgeschlagen: {e}')\n\n    # Drive sichern\n    if os.path.exists('/content/drive/MyDrive'):\n        import shutil\n        drive_dir = '/content/drive/MyDrive/BITCOIN4Traders/data'\n        os.makedirs(drive_dir, exist_ok=True)\n        for f in cache_dir.glob('*.parquet'):\n            shutil.copy2(str(f), os.path.join(drive_dir, f.name))\n            logger.info(f'Drive: {f.name} gespeichert')\n\nprint(f'\\nDatensatz: {len(price_data)} Zeilen')\nprint(f'Zeitraum: {price_data.index[0]} bis {price_data.index[-1]}')\nprice_data.head()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 9: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from features.feature_engine import FeatureEngine, FeatureConfig\n",
    "\n",
    "logger.info('Feature Engineering...')\n",
    "\n",
    "feature_config = FeatureConfig(\n",
    "    volatility_window=20,\n",
    "    ou_window=20,\n",
    "    rolling_mean_window=20,\n",
    "    use_log_returns=True,\n",
    "    scaler_type='standard',\n",
    "    save_scaler=True,\n",
    "    scaler_path=processed_dir,\n",
    "    dropna_strategy='rolling',\n",
    "    min_valid_rows=1000,\n",
    ")\n",
    "\n",
    "engine = FeatureEngine(feature_config)\n",
    "\n",
    "# Chronologischer Split: 70% Train, 15% Val, 15% Test\n",
    "n = len(price_data)\n",
    "train_idx = int(n * 0.70)\n",
    "val_idx = int(n * 0.85)\n",
    "\n",
    "train_data = price_data.iloc[:train_idx]\n",
    "val_data = price_data.iloc[train_idx:val_idx]\n",
    "test_data = price_data.iloc[val_idx:]\n",
    "\n",
    "logger.info(f'Split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}')\n",
    "\n",
    "# Fit NUR auf Trainingsdaten (kein Data Leakage!)\n",
    "logger.info('Fit FeatureEngine auf Trainingsdaten...')\n",
    "train_features = engine.fit_transform(train_data)\n",
    "\n",
    "logger.info('Transformiere Val und Test...')\n",
    "val_features = engine.transform(val_data)\n",
    "test_features = engine.transform(test_data)\n",
    "\n",
    "# Indizes angleichen\n",
    "common_train = train_data.index.intersection(train_features.index)\n",
    "train_price = train_data.loc[common_train]\n",
    "train_feat = train_features.loc[common_train]\n",
    "\n",
    "logger.success(f'Features berechnet: {train_feat.shape[1]} Features, {len(train_price)} Trainingssamples')\n",
    "print(f'Feature-Spalten: {list(train_feat.columns[:5])}...')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 10: Environment erstellen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from environment.config_integrated_env import ConfigIntegratedTradingEnv\n",
    "from environment.config_system import EnvironmentConfig, load_environment_config_from_yaml\n",
    "\n",
    "config_path = Path('config/environment/realistic_env.yaml')\n",
    "\n",
    "if config_path.exists():\n",
    "    env_config = load_environment_config_from_yaml(str(config_path))\n",
    "    logger.info('Environment-Config geladen')\n",
    "else:\n",
    "    env_config = EnvironmentConfig()\n",
    "    logger.warning('Verwende Standard-Config')\n",
    "\n",
    "env = ConfigIntegratedTradingEnv(train_price, train_feat, env_config)\n",
    "\n",
    "logger.success('Trading Environment erstellt')\n",
    "print(f'Observation Space: {env.observation_space.shape}')\n",
    "print(f'Action Space: {env.action_space.n}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 11: Trainer erstellen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from agents.ppo_agent import PPOConfig\n",
    "from training.adversarial_trainer import AdversarialTrainer, AdversarialConfig\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Trader (optimiert für Profit)\n",
    "trader_config = PPOConfig(\n",
    "    state_dim=state_dim,\n",
    "    hidden_dim=128,\n",
    "    n_actions=n_actions,\n",
    "    actor_lr=3e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10,\n",
    "    batch_size=64,\n",
    "    use_recurrent=True,\n",
    "    rnn_type='GRU',\n",
    "    entropy_coef=0.01,\n",
    "    value_loss_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=0.01,\n",
    ")\n",
    "\n",
    "# Adversary (erschafft schwierige Szenarien)\n",
    "adversary_config = PPOConfig(\n",
    "    state_dim=state_dim,\n",
    "    hidden_dim=128,\n",
    "    n_actions=n_actions,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=5e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10,\n",
    "    batch_size=64,\n",
    "    use_recurrent=True,\n",
    "    rnn_type='GRU',\n",
    "    entropy_coef=0.02,\n",
    ")\n",
    "\n",
    "# Training-Konfiguration\n",
    "training_config = AdversarialConfig(\n",
    "    n_iterations=N_ITERATIONS,\n",
    "    steps_per_iteration=STEPS_PER_ITER,\n",
    "    trader_config=trader_config,\n",
    "    adversary_config=adversary_config,\n",
    "    adversary_start_iteration=100,\n",
    "    adversary_strength=0.1,\n",
    "    save_frequency=SAVE_FREQUENCY,\n",
    "    log_frequency=10,\n",
    "    checkpoint_dir='data/models/adversarial',\n",
    ")\n",
    "\n",
    "trainer = AdversarialTrainer(env, training_config, device=DEVICE)\n",
    "\n",
    "logger.success('Trainer erstellt')\n",
    "print(f'State dim: {state_dim}, Actions: {n_actions}')\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'Iterationen: {N_ITERATIONS}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 12: [Optional] Von Checkpoint weitermachen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from loguru import logger\nimport os\nimport shutil\n\nDRIVE_MODEL_DIR = '/content/drive/MyDrive/BITCOIN4Traders/models'\nLOCAL_MODEL_DIR = '/content/BITCOIN4Traders/data/models/adversarial'\n\n# Prüfe ob Checkpoints auf Drive vorhanden\ndrive_checkpoints = []\nif os.path.exists(DRIVE_MODEL_DIR):\n    drive_checkpoints = sorted(\n        [f for f in os.listdir(DRIVE_MODEL_DIR) if f.endswith('.pth')]\n    )\n\nif drive_checkpoints:\n    latest = drive_checkpoints[-1]\n    src = os.path.join(DRIVE_MODEL_DIR, latest)\n    dst = os.path.join(LOCAL_MODEL_DIR, latest)\n    shutil.copy2(src, dst)\n    \n    logger.info(f'Lade Checkpoint: {latest}')\n    try:\n        trainer.load_checkpoint(dst)\n        logger.success(f'Checkpoint geladen: {latest}')\n    except Exception as e:\n        logger.error(f'Fehler beim Laden: {e}')\nelse:\n    logger.info('Kein Checkpoint gefunden - starte Training von Anfang an')\n\n# Manuell einen Checkpoint angeben:\n# RESUME_CHECKPOINT = 'data/models/adversarial/checkpoint_iter_200.pth'\n# if RESUME_CHECKPOINT and os.path.exists(RESUME_CHECKPOINT):\n#     trainer.load_checkpoint(RESUME_CHECKPOINT)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 13: Auto-Save Callback einrichten"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nimport shutil\nimport glob\n\ndef sync_models_to_drive():\n    \"\"\"Kopiert alle lokalen Checkpoints auf Google Drive.\"\"\"\n    local_dir = '/content/BITCOIN4Traders/data/models/adversarial'\n    drive_dir = '/content/drive/MyDrive/BITCOIN4Traders/models'\n    \n    checkpoints = glob.glob(os.path.join(local_dir, '*.pth'))\n    for cp in checkpoints:\n        fname = os.path.basename(cp)\n        dst = os.path.join(drive_dir, fname)\n        shutil.copy2(cp, dst)\n    \n    if checkpoints:\n        print(f'Drive sync: {len(checkpoints)} Checkpoint(s) gespeichert')\n\n# Test\nsync_models_to_drive()\nprint('Auto-Save Funktion bereit.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 14: TRAINING STARTEN\n",
    "\n",
    "> **Tipp:** Halte die Seite aktiv (z.B. Tab offen lassen) um Session-Timeouts zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "from loguru import logger\n",
    "\n",
    "logger.info('=' * 60)\n",
    "logger.info('TRAINING STARTET')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "_training_completed = False\n",
    "_current_step = 0\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    _training_completed = True\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning('Training unterbrochen (KeyboardInterrupt)')\n",
    "    logger.info('Speichere aktuellen Stand...')\n",
    "\n",
    "except RuntimeError as e:\n",
    "    error_str = str(e)\n",
    "    logger.error(f'RuntimeError: {error_str}')\n",
    "    if 'out of memory' in error_str.lower() or 'cuda' in error_str.lower():\n",
    "        import torch; torch.cuda.empty_cache()\n",
    "        send_help(error_str, error_type='CUDA_ERROR',\n",
    "                  extra={'step': _current_step, 'elapsed_h': round((time.time()-start_time)/3600, 2)})\n",
    "    else:\n",
    "        send_help(error_str, error_type='RuntimeError',\n",
    "                  extra={'step': _current_step})\n",
    "\n",
    "except MemoryError as e:\n",
    "    logger.error(f'MemoryError (RAM): {e}')\n",
    "    send_help(str(e), error_type='OOM',\n",
    "              extra={'step': _current_step})\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    logger.error(f'Unbekannter Fehler: {e}')\n",
    "    traceback.print_exc()\n",
    "    send_help(str(e), error_type='Exception',\n",
    "              extra={'step': _current_step, 'elapsed_h': round((time.time()-start_time)/3600, 2)})\n",
    "\n",
    "finally:\n",
    "    elapsed = (time.time() - start_time) / 3600\n",
    "    if _training_completed:\n",
    "        logger.success(f'Training abgeschlossen! Dauer: {elapsed:.1f}h')\n",
    "        report_status(_current_step, 0, extra={'status': 'completed', 'elapsed_h': elapsed})\n",
    "    else:\n",
    "        logger.info(f'Training gestoppt nach {elapsed:.1f}h')\n",
    "    # Immer auf Drive speichern!\n",
    "    logger.info('Synchronisiere mit Google Drive...')\n",
    "    sync_models_to_drive()\n",
    "    logger.success('Modell auf Drive gesichert!')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 15: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "logger.info('Evaluiere trainiertes Modell...')\n",
    "\n",
    "try:\n",
    "    metrics = trainer.evaluate(n_episodes=100)\n",
    "    \n",
    "    print('\\n=== Evaluationsergebnisse ===')\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f'  {key}: {value:.4f}')\n",
    "        else:\n",
    "            print(f'  {key}: {value}')\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f'Evaluation fehlgeschlagen: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 16: Checkpoints auf Drive anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "DRIVE_MODEL_DIR = '/content/drive/MyDrive/BITCOIN4Traders/models'\n",
    "\n",
    "print('Gespeicherte Modelle auf Google Drive:')\n",
    "print('=' * 50)\n",
    "\n",
    "if os.path.exists(DRIVE_MODEL_DIR):\n",
    "    files = sorted(os.listdir(DRIVE_MODEL_DIR))\n",
    "    total_mb = 0\n",
    "    for f in files:\n",
    "        path = os.path.join(DRIVE_MODEL_DIR, f)\n",
    "        size_mb = os.path.getsize(path) / 1e6\n",
    "        total_mb += size_mb\n",
    "        print(f'  {f:40s}  {size_mb:.1f} MB')\n",
    "    print(f'\\nGesamt: {len(files)} Dateien, {total_mb:.1f} MB')\n",
    "else:\n",
    "    print('Kein Modellordner auf Drive gefunden.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 17: Memory & Storage – Google Drive Sicherung\n",
    "\n",
    "Diese Zelle richtet die automatische Speicherung auf Google Drive ein und bietet Tools zum RAM-Aufräumen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from utils.memory_utils import cleanup_memory, save_and_trim_data, print_resource_report\n",
    "\n",
    "# RAM-Status prüfen\n",
    "print_resource_report()\n",
    "\n",
    "# Beispiel für die Nutzung:\n",
    "# if len(df) > 1500:\n",
    "#     df = save_and_trim_data(df, filename='trade_history.csv', max_rows=1000)\n",
    "print('Memory-Utilities bereit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 18: Multiverse Evolution — Darwin Engine\n",
    "\n",
    "Multiversum-Validierung: RSI, MACD, Bollinger und EMA treten in HUNDERTEN\n",
    "verschiedener Zeitlinien gegeneinander an (Monte Carlo + Stress-Szenarien).\n",
    "\n",
    "Szenarien:\n",
    "  - original       : Echte BTC-Daten unveraendert\n",
    "  - flash_crash    : Plotzlicher 30% Absturz + Teilerholen\n",
    "  - slow_bear      : Langsamer Baermarkt ueber alle Bars\n",
    "  - sideways_hell  : Nulldrift + hohes Rauschen (zerstoert Trend-Bots)\n",
    "  - parabolic_run  : Starker Bullenmarkt (testet ob Bot zu frueh aussteigt)\n",
    "  - mc_bull/bear/chop (50x je): Stochastische GBM-Zeitlinien\n",
    "\n",
    "Eliminierung: DD > 20% in IRGENDEINEM Szenario -> sofortige genetische Disqualifikation.\n",
    "Champion wird automatisch auf Drive gespeichert und beim naechsten Start geladen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===== MULTIVERSE EVOLUTION ENGINE =====\n",
    "import importlib.util, sys, os\n",
    "\n",
    "PROJECT_DIR = '/content/BITCOIN4Traders'\n",
    "SAVE_DIR = '/content/drive/MyDrive/BITCOIN4Traders/data'  # Drive-Persistenz\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "_darwin_path = os.path.join(PROJECT_DIR, 'darwin_engine.py')\n",
    "_spec = importlib.util.spec_from_file_location('darwin_engine', _darwin_path)\n",
    "_darwin = importlib.util.module_from_spec(_spec)\n",
    "_spec.loader.exec_module(_darwin)\n",
    "sys.modules['darwin_engine'] = _darwin\n",
    "\n",
    "from darwin_engine import run_multiverse, ChampionPersistence\n",
    "\n",
    "# ===== KONFIGURATION =====\n",
    "# Alle Werte aenderbar - keine hardcodierten Magic Numbers\n",
    "MV_GENERATIONS    = 15    # Generationen (mehr = robusterer Champion, langsamer)\n",
    "MV_POP_SIZE       = 20    # Bots pro Generation\n",
    "MV_MC_SCENARIOS   = 50    # Monte Carlo Zeitlinien pro Regime (bull/bear/chop je)\n",
    "MV_MAX_DD         = 0.20  # Max erlaubter Drawdown in JEDEM Szenario (20%)\n",
    "MV_N_BARS         = 2000  # Basis-Datenpunkte (mind. 500)\n",
    "MV_AUTO_LOAD      = True  # True = gespeicherten Champion laden falls vorhanden\n",
    "\n",
    "# Starte die Multiversum-Evolution:\n",
    "# 1. Laedt echte BTC/USDT Daten (Fallback: synthetisch)\n",
    "# 2. Generiert 5 + 3*MV_MC_SCENARIOS Szenarien\n",
    "# 3. Evolution: nur Ueberlebende aller Szenarien koennen sich reproduzieren\n",
    "# 4. Speichert Champion automatisch auf Drive nach jeder Generation\n",
    "multiverse_champion = run_multiverse(\n",
    "    symbol='BTC/USDT',\n",
    "    timeframe='1h',\n",
    "    n_bars=MV_N_BARS,\n",
    "    generations=MV_GENERATIONS,\n",
    "    pop_size=MV_POP_SIZE,\n",
    "    n_mc_scenarios=MV_MC_SCENARIOS,\n",
    "    max_dd_threshold=MV_MAX_DD,\n",
    "    auto_load_champion=MV_AUTO_LOAD,\n",
    "    save_dir=SAVE_DIR,\n",
    "    exchange_id='binance',\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "if multiverse_champion is not None:\n",
    "    print(f'\\nMultiverse Champion: {multiverse_champion.name}')\n",
    "    print(f'Strategie-Typ      : {type(multiverse_champion).__name__}')\n",
    "    print('Champion ist bereit fuer Zelle 19 (Risk Engine).')\n",
    "    # Metadaten anzeigen\n",
    "    meta_path = os.path.join(SAVE_DIR, 'multiverse_champion_meta.json')\n",
    "    ChampionPersistence.print_meta(meta_path)\n",
    "else:\n",
    "    print('Kein Champion hat alle Szenarien ueberlebt.')\n",
    "    print('Tipp: MV_MAX_DD erhoehen oder MV_GENERATIONS erhoehen.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 19: Risk Engine — Multiverse Champion zu Live-Trading\n",
    "\n",
    "Verbindet den Multiverse-Champion (aus Zelle 18) mit dem vollstaendigen\n",
    "Risk Management System: 1%-Regel, Kelly-Ceiling, ATR-Stop-Loss,\n",
    "Circuit-Breaker, Drawdown-Limit.\n",
    "\n",
    "Der Champion kann auch direkt von Drive geladen werden (kein Neustart noetig).\n",
    "Fuehrt eine simulierte TradingSession durch und zeigt den vollstaendigen Report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===== RISK ENGINE - Multiverse Champion zu Live-Trading-Session =====\n",
    "import importlib.util, sys, os\n",
    "\n",
    "PROJECT_DIR = '/content/BITCOIN4Traders'\n",
    "SAVE_DIR = '/content/drive/MyDrive/BITCOIN4Traders/data'\n",
    "\n",
    "# Risk Engine laden\n",
    "_risk_path = os.path.join(PROJECT_DIR, 'risk_engine.py')\n",
    "_spec = importlib.util.spec_from_file_location('risk_engine', _risk_path)\n",
    "_risk = importlib.util.module_from_spec(_spec)\n",
    "_spec.loader.exec_module(_risk)\n",
    "sys.modules['risk_engine'] = _risk\n",
    "\n",
    "# Darwin Engine laden (fuer ChampionPersistence)\n",
    "if 'darwin_engine' not in sys.modules:\n",
    "    _darwin_path = os.path.join(PROJECT_DIR, 'darwin_engine.py')\n",
    "    _spec2 = importlib.util.spec_from_file_location('darwin_engine', _darwin_path)\n",
    "    _darwin = importlib.util.module_from_spec(_spec2)\n",
    "    _spec2.loader.exec_module(_darwin)\n",
    "    sys.modules['darwin_engine'] = _darwin\n",
    "\n",
    "from risk_engine import run_full_pipeline, RiskConfig\n",
    "from darwin_engine import ChampionPersistence\n",
    "\n",
    "# ===== OPTION A: Champion aus Zelle 18 nutzen =====\n",
    "# (funktioniert wenn Zelle 18 in dieser Session ausgefuehrt wurde)\n",
    "champion_to_use = globals().get('multiverse_champion', None)\n",
    "\n",
    "# ===== OPTION B: Champion von Drive laden (nach Neustart) =====\n",
    "if champion_to_use is None:\n",
    "    print('Kein Champion in Session - lade von Drive...')\n",
    "    champion_to_use = ChampionPersistence.load(\n",
    "        os.path.join(SAVE_DIR, 'multiverse_champion.pkl'),\n",
    "        os.path.join(SAVE_DIR, 'multiverse_champion_meta.json'),\n",
    "    )\n",
    "\n",
    "if champion_to_use is None:\n",
    "    print('Kein gespeicherter Champion gefunden. Bitte zuerst Zelle 18 ausfuehren.')\n",
    "else:\n",
    "    print(f'Champion geladen: {champion_to_use.name} ({type(champion_to_use).__name__})')\n",
    "\n",
    "    # ===== RISK-KONFIGURATION =====\n",
    "    risk_cfg = RiskConfig(\n",
    "        initial_capital=10_000.0,   # Startkapital in USD\n",
    "        risk_per_trade=0.01,        # 1%-Regel: max 1% pro Trade riskieren\n",
    "        kelly_fraction=0.5,         # Half-Kelly (konservativ)\n",
    "        rr_ratio=2.0,               # Risk-Reward Ratio (1:2)\n",
    "        atr_sl_multiplier=2.0,      # Stop-Loss = Entry +/- 2x ATR\n",
    "        fee_rate=0.001,             # 0.1% Handelsgebuehr (Binance Taker)\n",
    "        slippage_rate=0.0005,       # 0.05% Slippage\n",
    "        max_daily_loss_pct=0.05,    # Circuit-Breaker: >5% Tagesverlust -> Stopp\n",
    "        max_consecutive_losses=3,   # Circuit-Breaker: 3 Verluste hintereinander\n",
    "        max_drawdown_pct=0.15,      # Drawdown-Limit: >15% -> Stopp\n",
    "        min_capital_pct=0.30,       # Min-Kapital-Gate: <30% Startkapital\n",
    "    )\n",
    "\n",
    "    # ===== PIPELINE STARTEN =====\n",
    "    # Tournament -> LiveTradingGuard (6 Gates) -> RiskEngine -> TradingSession\n",
    "    session_report = run_full_pipeline(\n",
    "        symbol='BTC/USDT',\n",
    "        timeframe='1h',\n",
    "        n_bars=1500,\n",
    "        risk_config=risk_cfg,\n",
    "        n_generations=5,\n",
    "        population_size=16,\n",
    "        n_wfv_splits=3,\n",
    "    )\n",
    "\n",
    "    if session_report is not None:\n",
    "        session_report.print_summary()\n",
    "    else:\n",
    "        print('Pipeline gestoppt - kein Champion hat alle 6 Sicherheits-Gates bestanden.')\n",
    "        print('Kein Trade ist besser als ein schlechter Trade.')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}