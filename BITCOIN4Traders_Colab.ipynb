{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BITCOIN4Traders - Google Colab Training\n",
    "\n",
    "**Anleitung:**\n",
    "1. Gehe zu `Runtime > Change runtime type` und wähle **GPU (T4)**\n",
    "2. Führe alle Zellen der Reihe nach aus\n",
    "3. Das Modell wird automatisch auf Google Drive gespeichert\n",
    "4. Bei Unterbrechung: Zelle 1-4 erneut ausführen, dann Resume-Zelle\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 1: GPU prüfen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU verfügbar: {gpu_name}')\n",
    "    print(f'GPU Speicher: {gpu_mem:.1f} GB')\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print('WARNUNG: Keine GPU gefunden! Gehe zu Runtime > Change runtime type > GPU')\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "print(f'Verwende Device: {DEVICE}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 2: Google Drive mounten (für persistente Speicherung)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive mounten\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Projektordner auf Drive erstellen\n",
    "DRIVE_PROJECT_DIR = '/content/drive/MyDrive/BITCOIN4Traders'\n",
    "DRIVE_MODEL_DIR = f'{DRIVE_PROJECT_DIR}/models'\n",
    "DRIVE_DATA_DIR = f'{DRIVE_PROJECT_DIR}/data'\n",
    "DRIVE_LOG_DIR = f'{DRIVE_PROJECT_DIR}/logs'\n",
    "\n",
    "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DRIVE_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DRIVE_LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Google Drive gemountet.')\n",
    "print(f'Modelle werden gespeichert in: {DRIVE_MODEL_DIR}')\n",
    "print(f'Daten werden gespeichert in: {DRIVE_DATA_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 3: Repository klonen / Projekt hochladen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n\nPROJECT_DIR = '/content/BITCOIN4Traders'\n\nif not os.path.exists(PROJECT_DIR):\n    print('Klone Projekt von GitHub...')\n    !git clone https://github.com/juancarlosrial76-code/BITCOIN4Traders.git {PROJECT_DIR}\n    print('Fertig!')\nelse:\n    print('Projekt bereits vorhanden. Aktualisiere...')\n    !git -C {PROJECT_DIR} pull\n    print('Aktualisiert!')\n\nprint(f'Arbeitsverzeichnis: {PROJECT_DIR}')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 4: Dependencies installieren"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\nprint('Installiere Dependencies...')\n\n!pip install -q ccxt loguru pyarrow pandas numpy scipy gymnasium stable-baselines3 ta yfinance numba hmmlearn scikit-learn pyyaml pydantic python-dotenv tqdm joblib matplotlib plotly omegaconf\n\n# Sicherstellen dass ccxt wirklich installiert ist\nimport importlib\nfor pkg in ['ccxt', 'loguru', 'pyarrow', 'gymnasium', 'omegaconf']:\n    try:\n        importlib.import_module(pkg)\n        print(f'  OK: {pkg}')\n    except ImportError:\n        print(f'  FEHLT: {pkg} - installiere nochmals...')\n        import subprocess\n        subprocess.run(['pip', 'install', '-q', pkg], check=True)\n\nprint('Installation abgeschlossen!')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 5: Python-Pfad setzen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_DIR = '/content/BITCOIN4Traders'\n",
    "SRC_DIR = os.path.join(PROJECT_DIR, 'src')\n",
    "\n",
    "# Pfade hinzufügen\n",
    "for path in [PROJECT_DIR, SRC_DIR]:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "# In Projektordner wechseln\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f'Arbeitsverzeichnis: {os.getcwd()}')\n",
    "print(f'Python-Pfad enthält: {SRC_DIR}')\n",
    "\n",
    "# Notwendige Verzeichnisse erstellen\n",
    "dirs = [\n",
    "    'data/cache',\n",
    "    'data/processed', \n",
    "    'data/models/adversarial',\n",
    "    'logs/training'\n",
    "]\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('Verzeichnisse erstellt.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 6: Daten von Drive laden oder herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "DRIVE_DATA_DIR = '/content/drive/MyDrive/BITCOIN4Traders/data'\n",
    "LOCAL_CACHE_DIR = '/content/BITCOIN4Traders/data/cache'\n",
    "\n",
    "# Prüfen ob gecachte Daten auf Drive vorhanden\n",
    "drive_cache_files = []\n",
    "if os.path.exists(DRIVE_DATA_DIR):\n",
    "    drive_cache_files = [f for f in os.listdir(DRIVE_DATA_DIR) if f.endswith('.parquet')]\n",
    "\n",
    "if drive_cache_files:\n",
    "    print(f'Lade gecachte Daten von Drive: {drive_cache_files}')\n",
    "    for fname in drive_cache_files:\n",
    "        src = os.path.join(DRIVE_DATA_DIR, fname)\n",
    "        dst = os.path.join(LOCAL_CACHE_DIR, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f'  Kopiert: {fname}')\n",
    "    print('Daten erfolgreich von Drive geladen!')\n",
    "else:\n",
    "    print('Keine gecachten Daten auf Drive gefunden.')\n",
    "    print('Daten werden beim Training von Binance heruntergeladen.')\n",
    "    print('(Dies dauert einige Minuten beim ersten Start)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 7: Training-Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n\n# ===== TRAINING-EINSTELLUNGEN =====\n# Diese Werte kannst du anpassen\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Datensatz\nSYMBOL = 'BTC/USDT'        # Handelspaar\nTIMEFRAME = '1h'            # Zeitrahmen: 1m, 5m, 15m, 1h, 4h, 1d\nSTART_DATE = '2022-01-01'   # Startdatum (mehr Daten = besseres Training)\nEND_DATE = None              # None = bis heute\nEXCHANGE = 'binance'\n\n# Training\nN_ITERATIONS = 500           # Anzahl Trainingsiterationen\nSTEPS_PER_ITER = 2048        # Schritte pro Iteration\nSAVE_FREQUENCY = 25          # Speichern alle N Iterationen (öfter als Standard)\n\n# Checkpoint (für Resume)\nRESUME_CHECKPOINT = None     # Pfad zu Checkpoint, z.B. 'data/models/adversarial/checkpoint_iter_100.pth'\n\nprint('Konfiguration:')\nprint(f'  Device:     {DEVICE}')\nprint(f'  Symbol:     {SYMBOL}')\nprint(f'  Timeframe:  {TIMEFRAME}')\nprint(f'  Start:      {START_DATE}')\nprint(f'  Iterationen: {N_ITERATIONS}')\nprint(f'  Save alle:  {SAVE_FREQUENCY} Iterationen')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 8: Daten laden & Features berechnen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom loguru import logger\nimport sys, os, importlib.util\n\n# Pfade\nPROJECT_DIR = '/content/BITCOIN4Traders'\nSRC_DIR = os.path.join(PROJECT_DIR, 'src')\n\n# Prüfe ob Projekt vorhanden\nif not os.path.exists(SRC_DIR):\n    raise RuntimeError(f\"FEHLER: {SRC_DIR} nicht gefunden! Führe zuerst Zelle 2 (git clone) aus.\")\n\n# Prüfe ob ccxt_loader vorhanden\nccxt_loader_path = os.path.join(SRC_DIR, 'data', 'ccxt_loader.py')\nif not os.path.exists(ccxt_loader_path):\n    raise RuntimeError(f\"FEHLER: {ccxt_loader_path} nicht gefunden!\")\n\n# Prüfe ob ccxt installiert\ntry:\n    import ccxt\nexcept ImportError:\n    print(\"ccxt fehlt - installiere...\")\n    import subprocess\n    subprocess.run(['pip', 'install', '-q', 'ccxt'], check=True)\n    import ccxt\n\n# Logging\nlogger.remove()\nlogger.add(sys.stdout, format=\"{time:HH:mm:ss} | {level} | {message}\", level=\"INFO\")\n\n# Verzeichnisse mit absolutem Pfad\ncache_dir = Path(os.path.join(PROJECT_DIR, 'data', 'cache'))\nprocessed_dir = Path(os.path.join(PROJECT_DIR, 'data', 'processed'))\ncache_dir.mkdir(parents=True, exist_ok=True)\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\n# Daten laden\ncached_files = list(cache_dir.glob('*.parquet'))\n\nif cached_files:\n    logger.info(f'Lade gecachte Daten: {cached_files[0]}')\n    price_data = pd.read_parquet(cached_files[0])\n    logger.success(f'Geladen: {len(price_data)} Candles')\nelse:\n    logger.info(f'Lade Daten von {EXCHANGE}...')\n\n    # Lade ccxt_loader direkt über Dateipfad\n    spec = importlib.util.spec_from_file_location(\"ccxt_loader\", ccxt_loader_path)\n    ccxt_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(ccxt_module)\n    CCXTDataLoader = ccxt_module.CCXTDataLoader\n    DataLoaderConfig = ccxt_module.DataLoaderConfig\n\n    config = DataLoaderConfig(\n        exchange_id=EXCHANGE,\n        exchange_type='spot',\n        rate_limit_ms=100,\n        cache_dir=cache_dir,\n        processed_dir=processed_dir,\n        compression='snappy',\n    )\n    loader = CCXTDataLoader(config)\n    price_data = loader.download_and_cache(\n        symbol=SYMBOL,\n        timeframe=TIMEFRAME,\n        start_date=START_DATE,\n        end_date=END_DATE,\n        force_refresh=False,\n    )\n    logger.success(f'Heruntergeladen: {len(price_data)} Candles')\n\n    # Daten auf Drive sichern\n    import shutil\n    drive_data_dir = '/content/drive/MyDrive/BITCOIN4Traders/data'\n    if os.path.exists('/content/drive/MyDrive'):\n        os.makedirs(drive_data_dir, exist_ok=True)\n        for f in list(cache_dir.glob('*.parquet')):\n            dst = os.path.join(drive_data_dir, f.name)\n            shutil.copy2(str(f), dst)\n            logger.info(f'Drive gespeichert: {dst}')\n\nprint(f'\\nDatensatz: {len(price_data)} Zeilen')\nprint(f'Zeitraum: {price_data.index[0]} bis {price_data.index[-1]}')\nprice_data.head()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 9: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from features.feature_engine import FeatureEngine, FeatureConfig\n",
    "\n",
    "logger.info('Feature Engineering...')\n",
    "\n",
    "feature_config = FeatureConfig(\n",
    "    volatility_window=20,\n",
    "    ou_window=20,\n",
    "    rolling_mean_window=20,\n",
    "    use_log_returns=True,\n",
    "    scaler_type='standard',\n",
    "    save_scaler=True,\n",
    "    scaler_path=processed_dir,\n",
    "    dropna_strategy='rolling',\n",
    "    min_valid_rows=1000,\n",
    ")\n",
    "\n",
    "engine = FeatureEngine(feature_config)\n",
    "\n",
    "# Chronologischer Split: 70% Train, 15% Val, 15% Test\n",
    "n = len(price_data)\n",
    "train_idx = int(n * 0.70)\n",
    "val_idx = int(n * 0.85)\n",
    "\n",
    "train_data = price_data.iloc[:train_idx]\n",
    "val_data = price_data.iloc[train_idx:val_idx]\n",
    "test_data = price_data.iloc[val_idx:]\n",
    "\n",
    "logger.info(f'Split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}')\n",
    "\n",
    "# Fit NUR auf Trainingsdaten (kein Data Leakage!)\n",
    "logger.info('Fit FeatureEngine auf Trainingsdaten...')\n",
    "train_features = engine.fit_transform(train_data)\n",
    "\n",
    "logger.info('Transformiere Val und Test...')\n",
    "val_features = engine.transform(val_data)\n",
    "test_features = engine.transform(test_data)\n",
    "\n",
    "# Indizes angleichen\n",
    "common_train = train_data.index.intersection(train_features.index)\n",
    "train_price = train_data.loc[common_train]\n",
    "train_feat = train_features.loc[common_train]\n",
    "\n",
    "logger.success(f'Features berechnet: {train_feat.shape[1]} Features, {len(train_price)} Trainingssamples')\n",
    "print(f'Feature-Spalten: {list(train_feat.columns[:5])}...')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 10: Environment erstellen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from environment.config_integrated_env import ConfigIntegratedTradingEnv\n",
    "from environment.config_system import EnvironmentConfig, load_environment_config_from_yaml\n",
    "\n",
    "config_path = Path('config/environment/realistic_env.yaml')\n",
    "\n",
    "if config_path.exists():\n",
    "    env_config = load_environment_config_from_yaml(str(config_path))\n",
    "    logger.info('Environment-Config geladen')\n",
    "else:\n",
    "    env_config = EnvironmentConfig()\n",
    "    logger.warning('Verwende Standard-Config')\n",
    "\n",
    "env = ConfigIntegratedTradingEnv(train_price, train_feat, env_config)\n",
    "\n",
    "logger.success('Trading Environment erstellt')\n",
    "print(f'Observation Space: {env.observation_space.shape}')\n",
    "print(f'Action Space: {env.action_space.n}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 11: Trainer erstellen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from agents.ppo_agent import PPOConfig\n",
    "from training.adversarial_trainer import AdversarialTrainer, AdversarialConfig\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Trader (optimiert für Profit)\n",
    "trader_config = PPOConfig(\n",
    "    state_dim=state_dim,\n",
    "    hidden_dim=128,\n",
    "    n_actions=n_actions,\n",
    "    actor_lr=3e-4,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10,\n",
    "    batch_size=64,\n",
    "    use_recurrent=True,\n",
    "    rnn_type='GRU',\n",
    "    entropy_coef=0.01,\n",
    "    value_loss_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=0.01,\n",
    ")\n",
    "\n",
    "# Adversary (erschafft schwierige Szenarien)\n",
    "adversary_config = PPOConfig(\n",
    "    state_dim=state_dim,\n",
    "    hidden_dim=128,\n",
    "    n_actions=n_actions,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=5e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    n_epochs=10,\n",
    "    batch_size=64,\n",
    "    use_recurrent=True,\n",
    "    rnn_type='GRU',\n",
    "    entropy_coef=0.02,\n",
    ")\n",
    "\n",
    "# Training-Konfiguration\n",
    "training_config = AdversarialConfig(\n",
    "    n_iterations=N_ITERATIONS,\n",
    "    steps_per_iteration=STEPS_PER_ITER,\n",
    "    trader_config=trader_config,\n",
    "    adversary_config=adversary_config,\n",
    "    adversary_start_iteration=100,\n",
    "    adversary_strength=0.1,\n",
    "    save_frequency=SAVE_FREQUENCY,\n",
    "    log_frequency=10,\n",
    "    checkpoint_dir='data/models/adversarial',\n",
    ")\n",
    "\n",
    "trainer = AdversarialTrainer(env, training_config, device=DEVICE)\n",
    "\n",
    "logger.success('Trainer erstellt')\n",
    "print(f'State dim: {state_dim}, Actions: {n_actions}')\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'Iterationen: {N_ITERATIONS}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 12: [Optional] Von Checkpoint weitermachen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from loguru import logger\nimport os\nimport shutil\n\nDRIVE_MODEL_DIR = '/content/drive/MyDrive/BITCOIN4Traders/models'\nLOCAL_MODEL_DIR = '/content/BITCOIN4Traders/data/models/adversarial'\n\n# Prüfe ob Checkpoints auf Drive vorhanden\ndrive_checkpoints = []\nif os.path.exists(DRIVE_MODEL_DIR):\n    drive_checkpoints = sorted(\n        [f for f in os.listdir(DRIVE_MODEL_DIR) if f.endswith('.pth')]\n    )\n\nif drive_checkpoints:\n    latest = drive_checkpoints[-1]\n    src = os.path.join(DRIVE_MODEL_DIR, latest)\n    dst = os.path.join(LOCAL_MODEL_DIR, latest)\n    shutil.copy2(src, dst)\n    \n    logger.info(f'Lade Checkpoint: {latest}')\n    try:\n        trainer.load_checkpoint(dst)\n        logger.success(f'Checkpoint geladen: {latest}')\n    except Exception as e:\n        logger.error(f'Fehler beim Laden: {e}')\nelse:\n    logger.info('Kein Checkpoint gefunden - starte Training von Anfang an')\n\n# Manuell einen Checkpoint angeben:\n# RESUME_CHECKPOINT = 'data/models/adversarial/checkpoint_iter_200.pth'\n# if RESUME_CHECKPOINT and os.path.exists(RESUME_CHECKPOINT):\n#     trainer.load_checkpoint(RESUME_CHECKPOINT)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 13: Auto-Save Callback einrichten"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nimport shutil\nimport glob\n\ndef sync_models_to_drive():\n    \"\"\"Kopiert alle lokalen Checkpoints auf Google Drive.\"\"\"\n    local_dir = '/content/BITCOIN4Traders/data/models/adversarial'\n    drive_dir = '/content/drive/MyDrive/BITCOIN4Traders/models'\n    \n    checkpoints = glob.glob(os.path.join(local_dir, '*.pth'))\n    for cp in checkpoints:\n        fname = os.path.basename(cp)\n        dst = os.path.join(drive_dir, fname)\n        shutil.copy2(cp, dst)\n    \n    if checkpoints:\n        print(f'Drive sync: {len(checkpoints)} Checkpoint(s) gespeichert')\n\n# Test\nsync_models_to_drive()\nprint('Auto-Save Funktion bereit.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 14: TRAINING STARTEN\n",
    "\n",
    "> **Tipp:** Halte die Seite aktiv (z.B. Tab offen lassen) um Session-Timeouts zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "logger.info('=' * 60)\n",
    "logger.info('TRAINING STARTET')\n",
    "logger.info('=' * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Training ausführen\n",
    "    trainer.train()\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 3600\n",
    "    logger.success(f'Training abgeschlossen! Dauer: {elapsed:.1f} Stunden')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning('Training unterbrochen (KeyboardInterrupt)')\n",
    "    logger.info('Speichere aktuellen Stand...')\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f'Fehler: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Immer auf Drive speichern!\n",
    "    logger.info('Synchronisiere mit Google Drive...')\n",
    "    sync_models_to_drive()\n",
    "    logger.success('Modell auf Drive gesichert!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 15: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "logger.info('Evaluiere trainiertes Modell...')\n",
    "\n",
    "try:\n",
    "    metrics = trainer.evaluate(n_episodes=100)\n",
    "    \n",
    "    print('\\n=== Evaluationsergebnisse ===')\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f'  {key}: {value:.4f}')\n",
    "        else:\n",
    "            print(f'  {key}: {value}')\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f'Evaluation fehlgeschlagen: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zelle 16: Checkpoints auf Drive anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "DRIVE_MODEL_DIR = '/content/drive/MyDrive/BITCOIN4Traders/models'\n",
    "\n",
    "print('Gespeicherte Modelle auf Google Drive:')\n",
    "print('=' * 50)\n",
    "\n",
    "if os.path.exists(DRIVE_MODEL_DIR):\n",
    "    files = sorted(os.listdir(DRIVE_MODEL_DIR))\n",
    "    total_mb = 0\n",
    "    for f in files:\n",
    "        path = os.path.join(DRIVE_MODEL_DIR, f)\n",
    "        size_mb = os.path.getsize(path) / 1e6\n",
    "        total_mb += size_mb\n",
    "        print(f'  {f:40s}  {size_mb:.1f} MB')\n",
    "    print(f'\\nGesamt: {len(files)} Dateien, {total_mb:.1f} MB')\n",
    "else:\n",
    "    print('Kein Modellordner auf Drive gefunden.')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}